---
title: "Investigating Influncial Features on Coffee Quality"
output: 
  html_document:
    df_print: paged
    number_sections: yes
---

```{r loadpackages, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
library(tidyverse)
library(gridExtra)
library(kableExtra)
library(jtools)
library(sjPlot)
library(car)
library(skimr)
library(corrplot)
```


# introduction

## Research question

What influence do different features of coffee have on whether the quality of a batch of coffee is classified as good or poor?

## Data description

The dataset is collected from the Coffee Quality Database (CQD) of Coffee Quality Institute. As a non-profit organisation, the institute aims to improve the quality of coffee and the lives of farmers who produce the beans. The dataset contains information on features of coffee and its production, including an overall quality score:  
 
* `country_of_origin` – Country where the coffee bean originates from.
* `aroma` – Aroma grade (ranging from 0-10.)
* `flavor` – Flavour grade (ranging from 0-10.)
* `acidity` – Acidity grade (ranging from 0-10.) 
* `category_two_defects` – Count of category 2 type defects in the batch of coffee beans tested.
* `altitiude_mean_meters` – Mean altitude of the growers farm (in meters.)
* `harvested` – Year the batch was harvested.
* `Qualityclass` – Quality score for the batch (Good - >= 82.5, Poor - < 82.5). (**Note:** 82.5 was selected as the cut off as this is the median score for all the batches tested.)


# Explanatory Data Analysis

The dataset can be loaded in and the first few lines can be viewed using the 'head' function. 

```{r data, echo=FALSE, eval=TRUE, warning=TRUE}
coffee <- read.csv("dataset13.csv")
head(coffee,n=10)
```

We will look at `Qualityclass` as our binary response variable (Poor/Good) and select all numerical variables as explanatory variables. 

## Data summarization 

The summary statistics are tabled below:

```{r summary, echo=FALSE, eval=TRUE}
coffee %>%
  select(-c(1,8)) %>%
  skim() %>%
  select(c(2,3,5,6,7,9,11)) %>%
  kable(col.names=c("Variables","Missing","Mean","SD","Min","Median","Max"),
        digits=2,
        booktabs=TRUE, 
        linesep="",  
        caption = 'Summary statistics on all numerical variables') %>%
  kable_styling(font_size=10, latex_options="HOLD_position")
```

From the table above, we notice that there are 201 missing observations from the `altitude_mean_meters` variable and 60 missing observations from the `harvested` variable. In addition, the variable of `altitude_mean_meters` has the largest variability which is 9392.09, and the maximum value is 190164 meters; this is out of question since the altitude of the highest mountain (Everest) which is about 8848 meters in the world.

It's also worth noting that some coffee beans get zero in the judgment of their features (`aroma`, `flavour`, `acidity`). We will plot histograms to show distributions of these features. 

```{r hist, echo = FALSE, eval = TRUE, warning = FALSE, fig.width=12, fig.height=6, fig.align = "center", fig.pos = "H", warning = FALSE, fig.cap = "Histogram of variables aroma, flavor and acidity"}
p1 <- ggplot(coffee, aes(x=aroma))+
  geom_histogram(color = "white")
p2 <- ggplot(coffee, aes(x=flavor))+
  geom_histogram(color = "white")
p3 <- ggplot(coffee, aes(x=acidity))+
  geom_histogram(color = "white")
grid.arrange(p1, p2, p3, ncol = 2)
```

These histograms show that most of coffee beans get grades between 6 and 8, so we can delete the observation with zero grades which is Honduras. 

## Data cleaning 

After exploring the summary statistics of the data, we decide to delete all the missing observations, the observations with value 0 of `aroma`, `acidity` and `flavor` and the observations with `altitude_mean_meters` greater than 8848 meters.

```{r delete, echo=FALSE, eval=TRUE}
data <- coffee %>%
  select(-c(1)) %>%
  na.omit() %>% 
  filter(aroma > 0,
         flavor > 0,
         acidity > 0,
         altitude_mean_meters < 8848)
```

And we set the baseline category for our binary response to `poor`.

```{r level, echo=FALSE, eval=TRUE}
data$Qualityclass<-factor(data$Qualityclass,level=c("Poor","Good"))
```

The final version of out data set will be like this:

```{r final, echo=FALSE,eval=TRUE}
str(data)
```

## Data visualization

The boxplots of `Qualityclass` against the six explanatory variables are plotted below : 

```{r boxplots, echo=FALSE, eval=TRUE, fig.cap="Boxplot of the Qualityclass against the other feature variables.",warning=FALSE,fig.pos="H",fig.align='center'}
p1 <- ggplot(data = data, aes(x = Qualityclass, y = aroma)) +
  geom_boxplot() +
  labs(x = "quality class", y = "aroma") + 
  theme(legend.position = "none")
p2 <- ggplot(data = data, aes(x = Qualityclass, y = flavor)) +
  geom_boxplot() +
  labs(x = "quality class", y = "flavor") + 
  theme(legend.position = "none")
p3 <- ggplot(data = data, aes(x = Qualityclass, y = acidity)) +
  geom_boxplot() +
  labs(x = "quality class", y = "acidity") + 
  theme(legend.position = "none")
p4 <- ggplot(data = data, aes(x = Qualityclass, y = category_two_defects)) +
  geom_boxplot() +
  labs(x = "quality class", y = "category_two_defects") + 
  theme(legend.position = "none")
p5 <- ggplot(data = data, aes(x = Qualityclass, y = altitude_mean_meters)) +
  geom_boxplot() +
  labs(x = "quality class", y = "altitude_mean_meters") + 
  theme(legend.position = "none")
p6<-ggplot(data = data, aes(x = Qualityclass, y = harvested)) +
  geom_boxplot()+
  labs(x = "quality class", y = "harvested") + 
  theme(legend.position = "none")
grid.arrange(p1, p2, p3, p4, p5, p6, ncol=3)
```

From the boxplots, we can see that the difference between quality class of good and poor against aroma, acidity and flavor is obvious since the the boxplots do not overlap with each other. However, it seems that there is no significant difference between quality class against the other variables. Then we would expect that aroma, acidity and flavor will have a strong influence to the quality class of a certain batch of coffee.

The correlation plot of all numerical variables is plotted below:

```{r corrp, echo=FALSE, eval=TRUE, warning=FALSE, fig.pos="H", fig.align='center', fig.cap="Correlation matrix plot of numerical variables"}
corrplot(cor(data[,-7]),method = "number")
```

From the correlation matrix plot, we notice a strong correlation between aroma, flavor and acidity, with the correlation coefficients of 0.73, 0.74 and 0.59, respectively. This can be explained by that these three variables are the basic standard to judge if the quality of a certain batch of coffee is good or poor. To fit our model well, we think the problem of multicollinearity should be considered, and we will discuss this issue in the next section.


# Formal Analysis

Our generalised linear model is generated by a binary response variable $y_i$ with the logit link function $g(·)$, that is

$$ y_i \sim Bin(1,p_i), $$
$$ g(p_i) = log\left(\frac{p_i}{1-p_i}\right) = \alpha + \sum_{i=1}^n \beta_i  x_i, $$

where,

* $y_i$ is the binary response variable;

* $p_i$ is the probability of being good (coffee beans);

* $1-p_i$ is the probability of being poor (coffee beans);
  
* $\alpha$ is the intercept;
  
* $\beta_i$ is the coefficient for the ith explanatory variable $x_i$;

* $x_i$ is the ith observation of the explanatory variable;
  
* $n$ is the number of explanatory variables.

## Multicollinearity

Firstly, we fit the logistic regression model with `Qualityclass` as the response and others as the explanatory variable. 

```{r model1, echo = FALSE, eval = TRUE, warning = FALSE}
dat1 <- data
mod1 <- glm(Qualityclass ~ ., data = dat1, family = binomial(link = "logit"))
```

We will calculate variance inflation factors (VIF) to detect multicollinearity of variables. The standard rule is that if VIF is greater than 10 then the multicollinearity is high.

```{r vif, echo = FALSE, eval = TRUE, warning = FALSE}
vif(mod1) %>%
  kable(col.names = "VIF", 
        digits = 3, 
        align = "c",
        caption = "\\label Variance inflation factor (VIF)",
        booktabs = TRUE) %>%
  kable_styling(font_size=10, latex_options="HOLD_position")
```

Since all VIF values are less than 10, we will initially use all six explanatory variables and fit the full model.

## Log-odds

**model 1**

Let's explore the significance of the coefficients.

```{r modsum, echo = FALSE, eval = TRUE, warning = FALSE}
summ(mod1)
```

The p-values of coefficients on `category_two_defects` (0.89) and `harvested` (0.17) are greater than 0.05, which means the two terms do not contribute significantly to this model. Let's remove `category_two_defects` first since it has larger p-value.

**model 2**

Remove the variable `category_two_defects`.

```{r model2, echo = FALSE, eval = TRUE, warning = FALSE}
dat2 <- dat1[,-4]
mod2 <- glm(Qualityclass ~ ., data = dat2, family = binomial(link = "logit"))
summ(mod2)
```

The p-value of the `harvested` variable (0.17) is still greater than 0.05, suggesting that it's not significant, so we will remove it from the model.  

**model 3**

Remove the variable `harvested`.

```{r model3, echo = FALSE, eval = TRUE, warning = FALSE}
dat3 <- dat2[,-5]
mod3 <- glm(Qualityclass ~ ., data = dat3, family = binomial(link = "logit"))
summ(mod3)
```

Now all coefficients are significant and we are interested in producing a 95% confidence interval for these log-odds. (**Note:** Due to the decimal places setting in summ function, the estimated value of `altitude_mean_meters` displays 0.00 rather than the actual value 0.0005.)

```{r lodds3, echo = FALSE, eval = TRUE, warning = FALSE, fig.pos = "H"}
confint(mod3) %>%
  kable(digits = 3, 
        align = "c",
        caption = "95% confidence interval for log-odds",
        booktabs = TRUE) %>%
  kable_styling(font_size=10, latex_options="HOLD_position")
plot_model(mod3, show.values = TRUE, transform = NULL, 
           title = "Log-Odds (Good quality of coffee)", show.p = FALSE)
```

The bound of 95% confidence interval for `altitude_mean_meters` is almost zero, so we decide to remove this variable and check if a new model could have a better performance than model 3.

**model 4** 

Remove the variable `altitude_mean_meters`.

```{r model4, echo = FALSE, eval = TRUE, warning = FALSE}
dat4 <- dat3[,-4]
mod4 <- glm(Qualityclass ~ ., data = dat4, family = binomial(link = "logit"))
summ(mod4)
```

Produce a 95% confidence interval for these log-odds. 

```{r lodds4, echo = FALSE, eval = TRUE, warning = FALSE, fig.pos = "H"}
confint(mod4) %>%
  kable(digits = 3, 
        align = "c",
        caption = "95% confidence interval for log-odds",
        booktabs = TRUE) %>%
  kable_styling(font_size=10, latex_options="HOLD_position")
plot_model(mod4, show.values = TRUE, transform = NULL,
             title = "Log-Odds (Good quality of coffee)", show.p = FALSE)
```

Now for model 4, all confidence intervals are significantly different from zero, and potentially we will keep this model as the candidate of the final model.

## Model selection

We will calculate AIC and BIC for the four models above and prefer one which minimizes AIC and BIC.

```{r comp, echo = FALSE, eval = TRUE, warning = FALSE}
model.comp1 <- glance(mod1)
model.comp2 <- glance(mod2)
model.comp3 <- glance(mod3)
model.comp4 <- glance(mod4)
Models <- c("GLM1","GLM2","GLM3","GLM4")
bind_rows(model.comp1, model.comp2, model.comp3, model.comp4, .id="Model") %>%
  select(Model,AIC,BIC) %>%
  mutate(Model=Models) %>%
  kable(digits = 3, 
        align = "c", 
        col.names = c("model","AIC","BIC"),
        caption = "Model comparison values for different models") %>%
  kable_styling(font_size=10, latex_options="HOLD_position")
```

From the table above,

* If we choose AIC as the selection criterion, model 3 will be the one that we are going to select as the final model with the value of 560.263;

* If we choose BIC as the selection criterion, model 4 will be the one that we will choose as the final model with the value of 583.957, but this value is close to the BIC value of model 3, which is 584.439.

We've removed `altitude_mean_meters` due to suspect confidence interval. Nevertheless, AIC value suggests that model 4 is not the best choice. It seems that `aroma`, `flavor` and `acidity` can not adequately explain quality class of different coffee beans, and some of the variations can be captured by `altitude_mean_meters`. With this in mind, it is proper to add `altitude_mean_meters` into the model and choose model 3 as the final one.

Now we can write our fitted model on the log-odds scale as

$$ log\left(\frac{p}{1-p}\right) = -121.42 + 4.53 \cdot aroma + 7.19 \cdot flavor + 4.27 \cdot acidity + 0.0005 \cdot altitude. $$

## Odds

We can obtain the odds by simply exponentiating the log-odds:

$$ \frac{p}{1-p} = exp(\alpha + \sum_{i=1}^n \beta_i x_i), $$

then

$$ \frac{p}{1-p} = exp(-121.42 + 4.53 \cdot aroma + 7.19 \cdot flavor + 4.27 \cdot acidity + 0.0005 \cdot altitude). $$

```{r odds, echo = FALSE, eval = TRUE, warning = FALSE, fig.pos = "H"}
mod3 %>%
  coef() %>%
  exp() %>%
  kable(digits = 3, 
        align = "c", 
        col.names = "Odds", 
        caption = "Odds (Good quality of coffee)") %>%
  kable_styling(font_size=10, latex_options="HOLD_position")
plot_model(mod3, show.values = TRUE, title = "Odds (Good quality of coffee)", show.p = FALSE)
```

From odds table, the coefficient of all covariates are all positive, indicating that good quality of coffee beans is highly likely to depend on the high scores of aroma, flavor, acidity and the high mean of altitude; however, the coefficient of mean of altitude is just above 1, suggesting that this is not a strongly influential element to determine the good quality of coffee beans. We can see that the odds-ratio of flavor is substantially larger than other covariates. The coefficient of this is 1326.59. This suggests that for good and bad quality differ by 1 flavor score, the higher flavor scores' odds of good quality are 1326.59 times those of lower flavor scores. Following that, the odds of aroma and acidity are approximately 93.075 and 71.27. These illustrate that for every 1 score increase in aroma and acidity, the odds of good quality of coffee beans rises by 93.075 and 71.27, respectively.

The odds plot depicts the point estimates and 95% confidence intervals. It is obvious that the confidence interval of flavor odds is the largest from about 266.9 to 7437. However, the confidence interval of the mean of altitude coefficient is the smallest; this indicates the high accuracy of this estimate. 

## Probabilities

To obtain the probability of being good quality of coffee beans, we can do the following transformation, that is

$$ p = \frac{exp(\alpha + \sum_{i=1}^n \beta_i x_i)}{1+exp(\alpha + \sum_{i=1}^n \beta_i x_i)}, $$

then

$$ p = \frac{exp(-121.42 + 4.53 \cdot aroma + 7.19 \cdot flavor + 4.27 \cdot acidity + 0.0005 \cdot altitude)}{1+exp(-121.42 + 4.53 \cdot aroma + 7.19 \cdot flavor + 4.27 \cdot acidity + 0.0005 \cdot altitude)}. $$

For example, a coffee bean has been ranked 8.3 in aroma, 7.9 in flavor, 7.3 in acidity, and grown at mean altitude of 1700 meters. We can obtain the probability of being good quality of this coffee bean.

```{r echo = FALSE}
x <- -121.42 + 4.53*8.3 + 7.19*7.9 + 4.27*7.3 + 0.0005*1700
p <- exp(x)/(1+exp(x))
```

$$ p = \frac{exp(-121.42 + 4.53 \cdot 8.3 + 7.19 \cdot 7.9 + 4.27 \cdot 7.3 + 0.0005 \cdot 1700)}{1+exp(-121.42 + 4.53 \cdot 8.3 + 7.19 \cdot 7.9 + 4.27 \cdot 7.3 + 0.0005 \cdot 1700)} = 0.993.$$ 

Finally, we can plot the probability of being good quality of coffee beans.

```{r prob, echo = FALSE, eval = TRUE, warning = FALSE, fig.pos = "H", fig.align = "center", fig.cap= "Probability of coffee being good"}
dat3 <- dat3 %>% mutate(probs.good = fitted(mod3))
p <- plot_model(mod3, type = "pred", title = "")
plot_grid(p)
```

The first three curved prediction lines show that the probability approaches 0 the smaller the explanatory variables get, and approaches 100% the larger the explanatory get. Although the plot of `altitude_mean_meters` follows the regularity, it approximates a line instead of a smooth curve.


# Conclusion

After fitting the four different models and using the AIC and BIC criteria to choose the optimal model to fit our data, model 3 (aroma, flavor, acidity and altitude as the explanatory variables) was selected. However, despite this model being selected, the fourth model that disregarded altitude should also be deemed as a potential model. The reasoning for this is the BIC and AIC values are both very close and the co-efficient and log-odds values are very weak for this particular explanatory variable. 

Looking at the odds for the explanatory variables of model 3, it clearly shows that the main three factors affecting the quality of coffee are aroma, flavor and acidity with flavor being the most influential factor. The higher score, the higher chance to be ranked good quality coffee beans. On the other hand, the higher altitude may not quite influence coffee bean quality since the co-efficient just above 1.   

Despite coming to this conclusion, our results demonstrate that the optimal model 3 is still far from being a perfect model and that further analysis may be required to investigate other influential factors to coffee quality. 


# Further Extension

There are many areas in which further study could be investigated. 

Upon initially reviewing the dataset it can be found several missing/impossible values within the dataset. To improve the results an investigation into why these values are missing data and how to find the data points. Along with the missing data, there is a number of impossible data points. For example, the highest the altitude could possibly be in meters is the top of Mount Everest (8848 meters above sea level), despite this in our data a number of values can be found above this level. A potential reason for this could be due to the poor quality of measuring equipment. 

Further work of this data could include some form of the spatial model being fitted to consider the geographical location of where the coffee was grown. As stated in the conclusion perhaps different explanatory variables could be explored to give a better representation of our data. These factors could focus on the soil that the coffee plant was grown in potentially looking at the PH of the soil or how tall the plant grew. 


# Reference

[1] Kutner, M. H.; Nachtsheim, C. J.; Neter, J. (2004). Applied Linear Regression Models (4th ed.). McGraw-Hill Irwin.

[2] ccs-amsterdam/r-course-material. GitHub. (2021). Retrieved 17 July 2021, from https://github.com/ccs-amsterdam/r-course-material/blob/master/tutorials/advanced_modeling.md#multilevel-models-or-mixed-effects-models.


